#!/usr/bin/env python3
# Developer: inkbytefo
# AI: Claude Sonnet 4.5
# Modified: 2025-11-01

"""
============================================================================
TMA-1 Preprocessing Script - Morfolojik Analizi √ñn ƒ∞≈üleme
Ham corpus'u okuyup her token i√ßin morfo-tip bilgisi ekler
============================================================================
"""

import argparse
import json
import os
from pathlib import Path
from tqdm import tqdm
from sentencepiece import SentencePieceProcessor
import sys
import random

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.morpho_splitter import MorphoSplitter

# Detaylƒ± morfem tipi mapping'i (23 kategori)
MORPHEME_TYPE_MAP = {
    'pad': 0,
    'special': 1,
    'isim_k√∂k': 2,
    'fiil_k√∂k': 3,
    'sƒ±fat_k√∂k': 4,
    'zarf_k√∂k': 5,
    'iyelik_1tekil': 6,
    'iyelik_2tekil': 7,
    'iyelik_3tekil': 8,
    'iyelik_1√ßoƒüul': 9,
    'iyelik_2√ßoƒüul': 10,
    'iyelik_3√ßoƒüul': 11,
    'belirtme': 12,
    'y√∂nelme': 13,
    'bulunma': 14,
    'ayrƒ±lma': 15,
    'ilgi': 16,
    'ge√ßmi≈ü_zaman': 17,
    '≈üimdiki_zaman': 18,
    'gelecek_zaman': 19,
    'geni≈ü_zaman': 20,
    '√ßoƒüul': 21,
    'other': 22
}

# Reverse mapping (ID -> name)
MORPHEME_TYPE_NAMES = {v: k for k, v in MORPHEME_TYPE_MAP.items()}

# Semantic category mapping (Anlamsal Kategoriler) - MORFOSEMANTIK TOKENIZATION
SEMANTIC_CATEGORY_MAP = {
    'pad': 0,
    'special': 1,
    # Mekan kategorisi (location)
    'mekan': 2,  # ev, okul, ≈üehir, park, sokak, bina, oda, vb.
    # Zaman kategorisi (time)
    'zaman': 3,  # g√ºn, ay, yƒ±l, saat, dakika, saniye, sabah, ak≈üam, vb.
    # ƒ∞nsan kategorisi (person)
    'insan': 4,  # adam, kadƒ±n, √ßocuk, √∂ƒüretmen, doktor, √∂ƒürenci, vb.
    # Hayvan kategorisi (animal)
    'hayvan': 5,  # kedi, k√∂pek, ku≈ü, at, inek, vb.
    # Duygu kategorisi (emotion)
    'duygu': 6,  # mutluluk, √ºz√ºnt√º, sevin√ß, korku, √∂fke, vb.
    # E≈üya kategorisi (object)
    'e≈üya': 7,  # masa, sandalye, kitap, kalem, telefon, araba, vb.
    # Yiyecek kategorisi (food)
    'yiyecek': 8,  # ekmek, su, √ßay, kahve, elma, vb.
    # Fiil kategorisi (action verbs)
    'fiil_eylem': 9,  # gitmek, gelmek, yapmak, okumak, yazmak, vb.
    # Sƒ±fat kategorisi (adjective)
    'sƒ±fat': 10,  # b√ºy√ºk, k√º√ß√ºk, g√ºzel, iyi, k√∂t√º, vb.
    # Bilinmeyen/anlamsal kategori belirlenemeyen
    'belirsiz': 11
}

# Basit keyword-based semantic categorizer
# ƒ∞leride WordNet veya daha geli≈ümi≈ü bir yapƒ± ile geni≈ületilebilir
SEMANTIC_KEYWORDS = {
    'mekan': {
        'ev', 'okul', '≈üehir', 'kent', 'park', 'sokak', 'cadde', 'bina', 'oda', 'salon',
        'mutfak', 'bah√ße', 'mahalle', 'il√ße', 'il', '√ºlke', 'k√∂y', 'kasaba', 'hastane',
        'k√ºt√ºphane', 'maƒüaza', 'd√ºkkan', 'ofis', 'fabrika', 'restoran', 'cafe', 'market',
        's√ºpermarket', 'okul', '√ºniversite', 'm√ºze', 'sinema', 'tiyatro', 'otopark'
    },
    'zaman': {
        'g√ºn', 'gece', 'sabah', '√∂ƒüle', 'ak≈üam', 'ay', 'yƒ±l', 'saat', 'dakika',
        'saniye', 'hafta', 'pazartesi', 'salƒ±', '√ßar≈üamba', 'per≈üembe', 'cuma', 'cumartesi',
        'pazar', 'ocak', '≈üubat', 'mart', 'nisan', 'mayƒ±s', 'haziran', 'temmuz', 'aƒüustos',
        'eyl√ºl', 'ekim', 'kasƒ±m', 'aralƒ±k', 'bug√ºn', 'd√ºn', 'yarƒ±n', 'sabah', '√∂ƒüleden',
        'ak≈üam√ºst√º', 'geceyarƒ±sƒ±'
    },
    'insan': {
        'adam', 'kadƒ±n', 'erkek', 'kƒ±z', '√ßocuk', 'bebek', '√∂ƒüretmen', 'doktor', '√∂ƒürenci',
        'anne', 'baba', 'karde≈ü', 'amca', 'dayƒ±', 'teyze', 'hala', 'nine', 'dede', 'b√ºy√ºkanne',
        'b√ºy√ºkbaba', 'hasta', 'm√ºhendis', 'avukat', 'polis', '√∂ƒüretmen', 'm√ºd√ºr', 'm√ºdire',
        'ba≈ükan', 'gen√ß', 'ya≈ülƒ±', 'yeti≈ükin'
    },
    'hayvan': {
        'kedi', 'k√∂pek', 'ku≈ü', 'at', 'inek', 'koyun', 'ke√ßi', 'tavuk', 'horoz', '√∂rdek',
        'balƒ±k', 'tav≈üan', 'fare', 'sƒ±√ßan', 'aslan', 'kaplan', 'ayƒ±', 'kurt', 'tilki',
        'domuz', 'at', 'e≈üek', 'deve', 'kuzu', 'oƒülak', 'yavru'
    },
    'e≈üya': {
        'masa', 'sandalye', 'kitap', 'kalem', 'defter', 'telefon', 'bilgisayar', 'araba',
        'bisiklet', 'motosiklet', 'g√∂mlek', 'pantolon', 'ayakkabƒ±', '√ßanta', 'anahtar',
        'kapƒ±', 'pencere', 'duvar', 'tavan', 'zemin', 'yol', 'k√∂pr√º', 'bina', 'ev',
        'televizyon', 'radyo', 'kamera', 'saat', 'tablo', 'resim'
    },
    'yiyecek': {
        'ekmek', 'su', '√ßay', 'kahve', 's√ºt', 'peynir', 'yumurta', 'et', 'tavuk', 'balƒ±k',
        'elma', 'armut', 'muz', 'portakal', 'domates', 'salatalƒ±k', 'soƒüan', 'patates',
        'pirin√ß', 'makarna', 'ekmek', 'b√∂rek', '√ß√∂rek', 'tatlƒ±', '√ßikolata', '≈üeker',
        'meyve', 'sebze', 'salata'
    },
    'fiil_eylem': {
        # Bu kategori genelde morfolojik analiz ile belirlenir, ama bazƒ± √∂rnekler:
        'git', 'gel', 'yap', 'oku', 'yaz', 'oku', 's√∂yle', 'd√º≈ü√ºn', 'anla', 'g√∂r',
        'bil', 'al', 'ver', 'sat', 'satƒ±n', '√∂ƒüren', '√∂ƒüret', '√ßalƒ±≈ü', 'oyna', 'ko≈ü'
    },
    'sƒ±fat': {
        'b√ºy√ºk', 'k√º√ß√ºk', 'g√ºzel', '√ßirkin', 'iyi', 'k√∂t√º', 'uzun', 'kƒ±sa', 'geni≈ü',
        'dar', 'y√ºksek', 'al√ßak', 'sƒ±cak', 'soƒüuk', 'sƒ±cak', 'hƒ±zlƒ±', 'yava≈ü', 'akƒ±llƒ±',
        'aptal', 'mutlu', '√ºzg√ºn', 'yeni', 'eski', 'temiz', 'kirli', 'a√ß', 'tok', 'susuz'
    }
}

def _detect_possessive_person(suffix: str) -> str:
    """
    ƒ∞yelik ekinin ≈üahƒ±s bilgisini ek string'inden √ßƒ±kar
    
    Args:
        suffix: ƒ∞yelik eki string'i (√∂rn: "im", "in", "leri")
    
    Returns:
        ≈ûahƒ±s bilgisi: "1tekil", "2tekil", "3tekil", "1√ßoƒüul", "2√ßoƒüul", "3√ßoƒüul", veya None
    """
    suffix_lower = suffix.lower().strip('-')
    
    # 1. tekil: -im, -ƒ±m, -um, -√ºm
    if suffix_lower in ['im', 'ƒ±m', 'um', '√ºm']:
        return '1tekil'
    
    # 2. tekil: -in, -ƒ±n, -un, -√ºn
    if suffix_lower in ['in', 'ƒ±n', 'un', '√ºn']:
        return '2tekil'
    
    # 3. tekil: -i, -ƒ±, -u, -√º, -si, -sƒ±, -su, -s√º
    if suffix_lower in ['i', 'ƒ±', 'u', '√º', 'si', 'sƒ±', 'su', 's√º', 'yi', 'yƒ±', 'yu', 'y√º']:
        return '3tekil'
    
    # 1. √ßoƒüul: -imiz, -ƒ±mƒ±z, -umuz, -√ºm√ºz
    if suffix_lower in ['imiz', 'ƒ±mƒ±z', 'umuz', '√ºm√ºz']:
        return '1√ßoƒüul'
    
    # 2. √ßoƒüul: -iniz, -ƒ±nƒ±z, -unuz, -√ºn√ºz
    if suffix_lower in ['iniz', 'ƒ±nƒ±z', 'unuz', '√ºn√ºz']:
        return '2√ßoƒüul'
    
    # 3. √ßoƒüul: -leri, -larƒ±
    if suffix_lower in ['leri', 'larƒ±']:
        return '3√ßoƒüul'
    
    return None

def get_detailed_morpho_type(token: str, morpho_splitter: MorphoSplitter) -> int:
    """
    Token'ƒ±n detaylƒ± morfolojik tipini belirle (23 kategori)
    
    Args:
        token: Token string
        morpho_splitter: MorphoSplitter instance
    
    Returns:
        Detaylƒ± morfolojik tip ID (0-22)
    """
    # √ñzel token'lar i√ßin
    if not token or token.strip() == "":
        return MORPHEME_TYPE_MAP['pad']
    
    if token.startswith("<") and token.endswith(">"):
        return MORPHEME_TYPE_MAP['special']
    
    # Morfolojik analiz yap
    analysis = morpho_splitter.split_word(token.strip())
    
    if not analysis['morfemler']:
        return MORPHEME_TYPE_MAP['other']
    
    # K√∂k kontrol√º: Eƒüer ek yoksa veya sadece k√∂k varsa
    if not analysis['ekler'] or len(analysis['morfemler']) == 1:
        root_morf = analysis['morfemler'][0]
        root_type = root_morf.get('t√ºr', 'k√∂k').lower()
        
        # K√∂k tipine g√∂re belirle
        if 'isim' in root_type or 'noun' in root_type:
            return MORPHEME_TYPE_MAP['isim_k√∂k']
        elif 'fiil' in root_type or 'verb' in root_type:
            return MORPHEME_TYPE_MAP['fiil_k√∂k']
        elif 'sƒ±fat' in root_type or 'adj' in root_type or 'adjective' in root_type:
            return MORPHEME_TYPE_MAP['sƒ±fat_k√∂k']
        elif 'zarf' in root_type or 'adv' in root_type or 'adverb' in root_type:
            return MORPHEME_TYPE_MAP['zarf_k√∂k']
        else:
            # Varsayƒ±lan olarak isim k√∂k
            return MORPHEME_TYPE_MAP['isim_k√∂k']
    
    # Ekler varsa, son ekten ba≈ülayarak kontrol et (en √∂nemli ek genelde sonda)
    # ƒ∞lk √∂nce zaman eklerine bak (fiiller i√ßin)
    for morf in reversed(analysis['morfemler'][1:]):  # K√∂k√º atla, sondan ba≈üla
        morf_type = morf.get('t√ºr', '').lower()
        morf_surface = morf.get('morfem', '').lower().strip('-')
        
        # Zaman ekleri (√∂ncelik: y√ºklem kontrol√º)
        if 'ge√ßmi≈ü_zaman' in morf_type or 'g√∂r√ºlen_ge√ßmi≈ü' in morf_type or 'past' in morf_type:
            return MORPHEME_TYPE_MAP['ge√ßmi≈ü_zaman']
        elif '≈üimdiki_zaman' in morf_type or 'prog' in morf_type or 'cont' in morf_type:
            return MORPHEME_TYPE_MAP['≈üimdiki_zaman']
        elif 'gelecek_zaman' in morf_type or 'fut' in morf_type:
            return MORPHEME_TYPE_MAP['gelecek_zaman']
        elif 'geni≈ü_zaman' in morf_type or 'aor' in morf_type:
            return MORPHEME_TYPE_MAP['geni≈ü_zaman']
        
        # √áoƒüul eki
        if '√ßoƒüul' in morf_type or morf_surface in ['ler', 'lar']:
            return MORPHEME_TYPE_MAP['√ßoƒüul']
        
        # Durum ekleri
        if 'belirtme' in morf_type or 'acc' in morf_type:
            return MORPHEME_TYPE_MAP['belirtme']
        elif 'y√∂nelme' in morf_type or 'dat' in morf_type:
            return MORPHEME_TYPE_MAP['y√∂nelme']
        elif 'bulunma' in morf_type or 'loc' in morf_type:
            return MORPHEME_TYPE_MAP['bulunma']
        elif 'ayrƒ±lma' in morf_type or 'abl' in morf_type:
            return MORPHEME_TYPE_MAP['ayrƒ±lma']
        elif 'ilgi' in morf_type or 'gen' in morf_type or morf_surface == 'ki':
            return MORPHEME_TYPE_MAP['ilgi']
        
        # ƒ∞yelik ekleri (≈üahƒ±s bilgisi ile)
        if 'iyelik' in morf_type or 'poss' in morf_type:
            person = _detect_possessive_person(morf_surface)
            if person:
                if person == '1tekil':
                    return MORPHEME_TYPE_MAP['iyelik_1tekil']
                elif person == '2tekil':
                    return MORPHEME_TYPE_MAP['iyelik_2tekil']
                elif person == '3tekil':
                    return MORPHEME_TYPE_MAP['iyelik_3tekil']
                elif person == '1√ßoƒüul':
                    return MORPHEME_TYPE_MAP['iyelik_1√ßoƒüul']
                elif person == '2√ßoƒüul':
                    return MORPHEME_TYPE_MAP['iyelik_2√ßoƒüul']
                elif person == '3√ßoƒüul':
                    return MORPHEME_TYPE_MAP['iyelik_3√ßoƒüul']
            # ≈ûahƒ±s belirlenemezse genel iyelik (3. tekil varsayƒ±lan)
            return MORPHEME_TYPE_MAP['iyelik_3tekil']
    
    # Hi√ßbir kategori bulunamazsa other
    return MORPHEME_TYPE_MAP['other']

# Geriye d√∂n√ºk uyumluluk i√ßin eski fonksiyon
def get_morpho_type(token: str, morpho_splitter: MorphoSplitter) -> int:
    """
    Token'ƒ±n morfolojik tipini belirle (GERƒ∞YE D√ñN√úK UYUMLULUK - eski 5 kategori)
    
    Args:
        token: Token string
        morpho_splitter: MorphoSplitter instance
    
    Returns:
        Morfolojik tip (0=root, 1=suffix, 2=verb, 3=other, 4=pad)
    """
    detailed_type = get_detailed_morpho_type(token, morpho_splitter)
    
    # Detaylƒ± tipten eski kategoriye d√∂n√º≈üt√ºr
    if detailed_type == MORPHEME_TYPE_MAP['pad']:
        return 4
    elif detailed_type == MORPHEME_TYPE_MAP['special']:
        return 3
    elif detailed_type in [MORPHEME_TYPE_MAP['ge√ßmi≈ü_zaman'], 
                           MORPHEME_TYPE_MAP['≈üimdiki_zaman'],
                           MORPHEME_TYPE_MAP['gelecek_zaman'],
                           MORPHEME_TYPE_MAP['geni≈ü_zaman']]:
        return 2  # Verb
    elif detailed_type in [MORPHEME_TYPE_MAP['isim_k√∂k'],
                           MORPHEME_TYPE_MAP['fiil_k√∂k'],
                           MORPHEME_TYPE_MAP['sƒ±fat_k√∂k'],
                           MORPHEME_TYPE_MAP['zarf_k√∂k']]:
        return 0  # Root
    else:
        return 1  # Suffix

def get_semantic_category(token: str, morpho_splitter: MorphoSplitter) -> int:
    """
    Token'ƒ±n anlamsal kategorisini belirle (MORFOSEMANTIK TOKENIZATION)
    
    Args:
        token: Token string
        morpho_splitter: MorphoSplitter instance (k√∂k bulmak i√ßin)
    
    Returns:
        Semantic category ID (0-11)
    """
    # √ñzel token'lar i√ßin
    if not token or token.strip() == "":
        return SEMANTIC_CATEGORY_MAP['pad']
    
    if token.startswith("<") and token.endswith(">"):
        return SEMANTIC_CATEGORY_MAP['special']
    
    # K√∂k√º bul (morfem analizi ile)
    analysis = morpho_splitter.split_word(token.strip())
    root = analysis.get('k√∂k', token.strip().lower())
    root_lower = root.lower()
    
    # Keyword-based matching (basit ama etkili)
    for category, keywords in SEMANTIC_KEYWORDS.items():
        if root_lower in keywords:
            return SEMANTIC_CATEGORY_MAP[category]
    
    # Morfolojik tip bilgisi kullanarak fallback
    morpho_type = get_detailed_morpho_type(token, morpho_splitter)
    
    # Fiil k√∂kleri i√ßin √∂zel kategori
    if morpho_type == MORPHEME_TYPE_MAP['fiil_k√∂k']:
        return SEMANTIC_CATEGORY_MAP['fiil_eylem']
    
    # Sƒ±fat k√∂kleri i√ßin √∂zel kategori
    if morpho_type == MORPHEME_TYPE_MAP['sƒ±fat_k√∂k']:
        return SEMANTIC_CATEGORY_MAP['sƒ±fat']
    
    # Bilinmeyen
    return SEMANTIC_CATEGORY_MAP['belirsiz']

def preprocess_corpus(
    input_file: str,
    output_file: str,
    tokenizer_path: str,
    morpho_splitter: MorphoSplitter,
    max_lines: int = None,
    split_dataset: bool = False,
    train_ratio: float = 0.8,
    val_ratio: float = 0.1,
    test_ratio: float = 0.1,
    random_seed: int = 42
) -> bool:
    """
    Corpus'u √∂n i≈üleme tabi tutup JSONL formatƒ±nda kaydet
    
    Args:
        input_file: Ham corpus dosyasƒ±
        output_file: √áƒ±ktƒ± JSONL dosyasƒ± (veya train/val/test dosyalarƒ± i√ßin base path)
        tokenizer_path: SentencePiece tokenizer model path
        morpho_splitter: MorphoSplitter instance
        max_lines: Maksimum i≈ülenecek satƒ±r sayƒ±sƒ± (None = hepsi)
        split_dataset: Eƒüer True ise, corpus'u train/val/test olarak b√∂l
        train_ratio: Train set oranƒ± (default: 0.8)
        val_ratio: Validation set oranƒ± (default: 0.1)
        test_ratio: Test set oranƒ± (default: 0.1)
        random_seed: Random seed for shuffling (default: 42)
    
    Returns:
        Ba≈üarƒ±lƒ± ise True
    """
    if not os.path.exists(input_file):
        print(f"‚ùå Input file not found: {input_file}")
        return False
    
    if not os.path.exists(tokenizer_path):
        print(f"‚ùå Tokenizer not found: {tokenizer_path}")
        print(f"üí° Train tokenizer first: python src/train_morphopiece.py --train")
        return False
    
    # Load tokenizer
    print(f"\nüì• Loading tokenizer: {tokenizer_path}")
    tokenizer = SentencePieceProcessor(model_file=tokenizer_path)
    print(f"‚úÖ Tokenizer loaded (vocab size: {tokenizer.vocab_size():,})")
    
    # Count lines in input file
    print(f"\nüìä Counting lines in corpus: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        total_lines = sum(1 for _ in f)
    
    if max_lines:
        total_lines = min(total_lines, max_lines)
    
    print(f"‚úÖ Found {total_lines:,} lines to process")
    
    # Determine output files based on split_dataset flag
    if split_dataset:
        # Split into train/val/test files
        base_path = output_file.replace('.jsonl', '') if output_file.endswith('.jsonl') else output_file
        train_file = f"{base_path}_train.jsonl"
        val_file = f"{base_path}_val.jsonl"
        test_file = f"{base_path}_test.jsonl"
        output_files = {
            'train': train_file,
            'val': val_file,
            'test': test_file
        }
        print(f"\nüìä Dataset will be split:")
        print(f"   Train: {train_file} ({train_ratio*100:.0f}%)")
        print(f"   Validation: {val_file} ({val_ratio*100:.0f}%)")
        print(f"   Test: {test_file} ({test_ratio*100:.0f}%)")
        
        # Validate ratios sum to 1.0
        if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
            print(f"‚ö†Ô∏è  Warning: Ratios sum to {train_ratio + val_ratio + test_ratio:.6f}, not 1.0")
            print(f"   Normalizing ratios...")
            total = train_ratio + val_ratio + test_ratio
            train_ratio /= total
            val_ratio /= total
            test_ratio /= total
        
        # Open all output files
        f_outs = {
            'train': open(train_file, 'w', encoding='utf-8'),
            'val': open(val_file, 'w', encoding='utf-8'),
            'test': open(test_file, 'w', encoding='utf-8')
        }
    else:
        output_files = {'all': output_file}
        f_outs = {'all': open(output_file, 'w', encoding='utf-8')}
    
    # Process corpus
    print(f"\nüîÑ Processing corpus...")
    print(f"   Input: {input_file}")
    
    processed_count = 0
    error_count = 0
    all_lines = []
    
    # First pass: Read all lines into memory (for shuffling if splitting)
    if split_dataset:
        print(f"üìñ Reading all lines into memory for shuffling...")
        with open(input_file, 'r', encoding='utf-8') as f_in:
            for line_idx, line in enumerate(tqdm(f_in, total=total_lines, desc="Reading")):
                if max_lines and line_idx >= max_lines:
                    break
                all_lines.append((line_idx, line.strip()))
        
        # Shuffle with seed for reproducibility
        print(f"üîÄ Shuffling {len(all_lines):,} lines (seed={random_seed})...")
        random.seed(random_seed)
        random.shuffle(all_lines)
        
        # Calculate split indices
        num_lines = len(all_lines)
        train_end = int(num_lines * train_ratio)
        val_end = train_end + int(num_lines * val_ratio)
        
        print(f"üìä Split indices: Train [0:{train_end:,}), Val [{train_end:,}:{val_end:,}), Test [{val_end:,}:{num_lines:,})")
        
        # Assign lines to splits
        lines_by_split = {
            'train': all_lines[:train_end],
            'val': all_lines[train_end:val_end],
            'test': all_lines[val_end:]
        }
        
        # Process each split
        for split_name, lines in lines_by_split.items():
            print(f"\nüîÑ Processing {split_name} set ({len(lines):,} lines)...")
            f_out = f_outs[split_name]
            
            for line_idx, line in tqdm(lines, desc=f"Processing {split_name}", unit="lines"):
                if not line:
                    # Bo≈ü satƒ±r i√ßin bo≈ü output
                    output = {
                        "tokens": [],
                        "morpho_types": [],
                        "semantic_categories": []
                    }
                    f_out.write(json.dumps(output, ensure_ascii=False) + '\n')
                    processed_count += 1
                    continue
                
                try:
                    # Tokenize
                    token_ids = tokenizer.encode(line, out_type=int)
                    
                    # Decode token IDs to get token strings
                    tokens = []
                    for token_id in token_ids:
                        token = tokenizer.id_to_piece(token_id)
                        # Remove SentencePiece prefix (‚ñÅ for space)
                        if token.startswith('‚ñÅ'):
                            token = token[1:]
                        if not token:
                            token = '<SPACE>'
                        tokens.append(token)
                    
                    # Get detailed morphological types and semantic categories for each token
                    morpho_types = []
                    semantic_categories = []
                    for token in tokens:
                        morpho_type = get_detailed_morpho_type(token, morpho_splitter)
                        semantic_category = get_semantic_category(token, morpho_splitter)
                        morpho_types.append(morpho_type)
                        semantic_categories.append(semantic_category)
                    
                    # Write JSONL line
                    output = {
                        "tokens": tokens,
                        "morpho_types": morpho_types,
                        "semantic_categories": semantic_categories
                    }
                    f_out.write(json.dumps(output, ensure_ascii=False) + '\n')
                    processed_count += 1
                    
                except Exception as e:
                    error_count += 1
                    print(f"\n‚ö†Ô∏è  Error processing {split_name} line {line_idx}: {e}")
                    # Error durumunda bo≈ü output yaz
                    output = {
                        "tokens": [],
                        "morpho_types": [],
                        "semantic_categories": []
                    }
                    f_out.write(json.dumps(output, ensure_ascii=False) + '\n')
    else:
        # Original behavior: single output file, streaming processing
        f_out = f_outs['all']
        with open(input_file, 'r', encoding='utf-8') as f_in:
            progress_bar = tqdm(total=total_lines, desc="Processing", unit="lines")
            
            for line_idx, line in enumerate(f_in):
                if max_lines and line_idx >= max_lines:
                    break
                
                line = line.strip()
                if not line:
                    # Bo≈ü satƒ±r i√ßin bo≈ü output
                    output = {
                        "tokens": [],
                        "morpho_types": [],
                        "semantic_categories": []
                    }
                    f_out.write(json.dumps(output, ensure_ascii=False) + '\n')
                    processed_count += 1
                    progress_bar.update(1)
                    continue
                
                try:
                    # Tokenize
                    token_ids = tokenizer.encode(line, out_type=int)
                    
                    # Decode token IDs to get token strings
                    tokens = []
                    for token_id in token_ids:
                        token = tokenizer.id_to_piece(token_id)
                        # Remove SentencePiece prefix (‚ñÅ for space)
                        if token.startswith('‚ñÅ'):
                            token = token[1:]
                        if not token:
                            token = '<SPACE>'
                        tokens.append(token)
                    
                    # Get detailed morphological types and semantic categories for each token
                    morpho_types = []
                    semantic_categories = []
                    for token in tokens:
                        morpho_type = get_detailed_morpho_type(token, morpho_splitter)
                        semantic_category = get_semantic_category(token, morpho_splitter)
                        morpho_types.append(morpho_type)
                        semantic_categories.append(semantic_category)
                    
                    # Write JSONL line
                    output = {
                        "tokens": tokens,
                        "morpho_types": morpho_types,
                        "semantic_categories": semantic_categories
                    }
                    f_out.write(json.dumps(output, ensure_ascii=False) + '\n')
                    processed_count += 1
                
                except Exception as e:
                    error_count += 1
                    print(f"\n‚ö†Ô∏è  Error processing line {line_idx}: {e}")
                    # Error durumunda bo≈ü output yaz
                    output = {
                        "tokens": [],
                        "morpho_types": [],
                        "semantic_categories": []
                    }
                    f_out.write(json.dumps(output, ensure_ascii=False) + '\n')
                
                progress_bar.update(1)
            
            progress_bar.close()
    
    # Close all output files
    for f_out in f_outs.values():
        f_out.close()
    
    print(f"\n‚úÖ Preprocessing completed!")
    print(f"   Processed: {processed_count:,} lines")
    print(f"   Errors: {error_count:,} lines")
    if split_dataset:
        print(f"   Output files:")
        for split_name, file_path in output_files.items():
            print(f"     {split_name}: {file_path}")
    else:
        print(f"   Output: {output_file}")
    
    return True

def main():
    parser = argparse.ArgumentParser(description='Preprocess corpus for TMA-1 training')
    
    parser.add_argument('--input', type=str, required=True,
                       help='Input corpus file (raw text, one line per sentence)')
    parser.add_argument('--output', type=str, required=True,
                       help='Output JSONL file (preprocessed with morpho types)')
    parser.add_argument('--tokenizer', type=str, required=True,
                       help='Path to SentencePiece tokenizer model (.model file)')
    parser.add_argument('--max-lines', type=int, default=None,
                       help='Maximum number of lines to process (default: all)')
    parser.add_argument('--use-java', action='store_true', default=False,
                       help='Use Zemberek Java (if available) for morphological analysis')
    parser.add_argument('--split-dataset', action='store_true', default=False,
                       help='Split corpus into train/val/test sets (80/10/10)')
    parser.add_argument('--train-ratio', type=float, default=0.8,
                       help='Training set ratio (default: 0.8)')
    parser.add_argument('--val-ratio', type=float, default=0.1,
                       help='Validation set ratio (default: 0.1)')
    parser.add_argument('--test-ratio', type=float, default=0.1,
                       help='Test set ratio (default: 0.1)')
    parser.add_argument('--random-seed', type=int, default=42,
                       help='Random seed for shuffling (default: 42)')
    
    args = parser.parse_args()
    
    # Create output directory if needed
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
    
    # Initialize MorphoSplitter
    print(f"\nüìù Initializing MorphoSplitter...")
    morpho_splitter = MorphoSplitter(use_java=args.use_java)
    
    # Preprocess corpus
    success = preprocess_corpus(
        input_file=args.input,
        output_file=args.output,
        tokenizer_path=args.tokenizer,
        morpho_splitter=morpho_splitter,
        max_lines=args.max_lines,
        split_dataset=args.split_dataset,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        random_seed=args.random_seed
    )
    
    if success:
        print(f"\n‚úÖ Preprocessing successful!")
        if args.split_dataset:
            base_path = args.output.replace('.jsonl', '') if args.output.endswith('.jsonl') else args.output
            train_file = f"{base_path}_train.jsonl"
            val_file = f"{base_path}_val.jsonl"
            test_file = f"{base_path}_test.jsonl"
            print(f"üí° Next step: Train TMA-1 model with:")
            print(f"   python train_tma1.py --corpus {train_file} --val-corpus {val_file} --tokenizer {args.tokenizer}")
            print(f"   Test set saved to: {test_file}")
        else:
            print(f"üí° Next step: Train TMA-1 model with:")
            print(f"   python train_tma1.py --corpus {args.output} --tokenizer {args.tokenizer}")
    else:
        print(f"\n‚ùå Preprocessing failed!")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())

