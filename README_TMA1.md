// Developer: inkbytefo
// AI: Claude Sonnet 4.5
// Modified: 2025-11-01

# ğŸ§  TMA-1: TÃ¼rkÃ§e MantÄ±k AÄŸÄ±

> **Morfem farkÄ±ndalÄ±klÄ±, eklemeli yapÄ±ya Ã¶zel transformer modeli**

TMA-1, TÃ¼rkÃ§e'nin eklemeli yapÄ±sÄ±nÄ± modelin DNA'sÄ±na yerleÅŸtiren, morfolojik farkÄ±ndalÄ±ÄŸa sahip bir transformer modelidir. Bu dokÃ¼man, proje yapÄ±sÄ±yla uyumlu, profesyonel kullanÄ±m talimatlarÄ± ve aÃ§Ä±klamalar iÃ§erir.

## ğŸ¯ TMA-1 Ã–zellikleri

### 1. Morfem AyrÄ±mÄ± (MorphoSplitter)
- Zemberek entegrasyonu ile kelime analizi
- KÃ¶k + ek ayrÄ±mÄ±
- ÃœnlÃ¼ uyumu kontrolÃ¼

### 2. MorphoPiece Tokenizer
- SentencePiece + morfem analizi kombinasyonu
- KÃ¶kler ve ekler ayrÄ± token'lar
- Morfoloji-aware eÄŸitim ve encoding opsiyonu

### 3. Agglutinative Attention
- SOV yapÄ±sÄ±na gÃ¶re Ã¶zel attention
- YÃ¼klem token'larÄ±na ekstra bias
- KÃ¶k ve ek token'larÄ±na farklÄ± aÄŸÄ±rlÄ±klar

### 4. Grammar Engine
- ÃœnlÃ¼ uyumu kurallarÄ±
- Ek sÄ±rasÄ± kontrolÃ¼
- Yasak kombinasyon tespiti
- **Vectorized logit bias sistemi** (PyTorch tensor operasyonlarÄ±, O(vocab_size) dÃ¶ngÃ¼sÃ¼ yok)
- **Vocabulary cache**: Vocabulary Ã¼nlÃ¼ bilgisi Ã¶nceden hesaplanÄ±p cache'leniyor
- **Performance**: 100-1000x hÄ±zlanma (tensor broadcasting vs Python dÃ¶ngÃ¼leri)

### 5. TMA-1 Model
- Morfem farkÄ±ndalÄ±klÄ± transformer
- Grammar-aware generation
- TÃ¼rkÃ§e'ye Ã¶zel mimari
- `AgglutinativeAttention` ve `GrammarEngine` ile logit/attention bias
- **Preprocessing optimizasyonu**: Morfolojik analiz eÄŸitim sÄ±rasÄ±nda yapÄ±lmÄ±yor, preprocessing'de Ã¶nceden hesaplanÄ±yor

## ğŸš€ KullanÄ±m

### Morfem AyrÄ±mÄ±

```python
from src.morpho_splitter import MorphoSplitter

splitter = MorphoSplitter()
result = splitter.split_word("Evlerimdekiler")

print(result)
# {
#   "kelime": "evlerimdekiler",
#   "kÃ¶k": "ev",
#   "ekler": ["ler", "im", "de", "ki", "ler"],
#   "morfemler": [...]
# }
```

### MorphoPiece Tokenizer

```python
from src.morphopiece import MorphoPiece

# EÄŸit
morphopiece = MorphoPiece()
morphopiece.train(
    corpus_file="data/corpus.txt",
    output_prefix="tokenizer/morphopiece",
    vocab_size=32000,
    morpho_aware=True
)

# Kullan
tokens = morphopiece.encode("DÃ¼n markete gittim", morpho_aware=True)
text = morphopiece.decode(tokens)
```

### TMA-1 Model

```python
from src.tma1_model import TMA1Model
from src.model import ModelConfig

config = ModelConfig(
    vocab_size=32000,
    hidden_size=768,
    num_layers=12,
    num_heads=12
)

model = TMA1Model(config)

# Forward pass
input_ids = torch.randint(0, 32000, (2, 10))
logits, _ = model(input_ids, vocab=vocab_list)
```

## ğŸ“Š Mimari KarÅŸÄ±laÅŸtÄ±rma

| Ã–zellik | Standard Transformer | TMA-1 |
|---------|---------------------|-------|
| Tokenization | Word-based / BPE | MorphoPiece (kÃ¶k+ek) |
| Attention | Standard | Agglutinative (SOV) |
| Grammar | None | Grammar Engine |
| Morfem Awareness | âŒ | âœ… |
| Vowel Harmony | âŒ | âœ… |
| Suffix Order | âŒ | âœ… |

## ğŸ”§ EÄŸitim

Komut satÄ±rÄ± Ã¶rnekleri:

```bash
# 1. MorphoPiece eÄŸitimi (morfem Ã¶n iÅŸlemeyle)
python src/train_morphopiece.py --preprocess --corpus-file data/test_corpus.txt --preprocessed-file data/corpus_morpho_processed.txt --train --output tokenizer/morphopiece --vocab-size 1000

# 2. TMA-1 iÃ§in corpus Ã¶n iÅŸleme (Ã–NEMLÄ° - 10-100x hÄ±zlanma saÄŸlar)
python scripts/preprocess_for_tma1.py \
    --input data/test_corpus.txt \
    --output data/train_data.jsonl \
    --tokenizer tokenizer/morphopiece.model

# 3. Baseline Transformer eÄŸitimi
python train.py --corpus data/test_corpus.txt --tokenizer tokenizer/morphopiece.model --output-dir models/baseline

# 4. TMA-1 eÄŸitimi (Ã¶n iÅŸlenmiÅŸ JSONL kullanarak - HIZLI)
python train_tma1.py --corpus data/train_data.jsonl --tokenizer tokenizer/morphopiece.model --output-dir models/tma1

# Not: Text formatÄ± kullanÄ±lÄ±rsa yavaÅŸ Ã§alÄ±ÅŸÄ±r (runtime morfolojik analiz)
python train_tma1.py --corpus data/test_corpus.txt --tokenizer tokenizer/morphopiece.model --output-dir models/tma1  # SLOW
```

## ğŸ“ Ã–rnek Ã‡Ä±ktÄ±

**Input:** "DÃ¼n ne yaptÄ±n?"

**Standard Model:** "DÃ¼nÃ¼ unuttum ama sanÄ±rÄ±m evdeydim."

**TMA-1:** "DÃ¼nÃ¼ unuttum ama sanÄ±rÄ±m markete gittim." âœ…

(TMA-1, Ã¼nlÃ¼ uyumu ve ek sÄ±rasÄ± kurallarÄ±na daha uygun Ã§Ä±ktÄ± Ã¼retir)

## ğŸ¯ Sonraki AdÄ±mlar

1. âœ… Morfem ayrÄ±mÄ± (Zemberek/regex fallback)
2. âœ… MorphoPiece tokenizer
3. âœ… Agglutinative attention
4. âœ… Grammar engine (**vectorized**)
5. âœ… TMA-1 model
6. âœ… **Preprocessing pipeline** (`preprocess_for_tma1.py`)
7. âœ… **JSONL dataset support** (morpho_types tensors)
8. âœ… Testler (`pytest -q`)
9. ğŸ”„ GeniÅŸ corpus ile uzun eÄŸitim
10. ğŸ”„ DeÄŸerlendirme metrikleri ve fine-tuning

## âš¡ Performans OptimizasyonlarÄ±

### Preprocessing (AdÄ±m 1)
- **Sorun**: EÄŸitim sÄ±rasÄ±nda her forward pass'te morfolojik analiz yapÄ±lÄ±yordu (Ã§ok yavaÅŸ)
- **Ã‡Ã¶zÃ¼m**: Morfolojik analiz preprocessing'de yapÄ±lÄ±yor, sonuÃ§lar JSONL'de cache'leniyor
- **KazanÃ§**: 10-100x eÄŸitim hÄ±zlanmasÄ±

### Vectorized Grammar Bias (AdÄ±m 2)
- **Sorun**: `apply_grammar_bias()` her pozisyon iÃ§in tÃ¼m vocabulary Ã¼zerinde dÃ¶ngÃ¼ yapÄ±yordu (O(vocab_size))
- **Ã‡Ã¶zÃ¼m**: PyTorch tensor broadcasting ve mask operasyonlarÄ± kullanÄ±lÄ±yor
- **KazanÃ§**: 100-1000x grammar bias hÄ±zlanmasÄ±

### Toplam Performans
- EÄŸitim hÄ±zÄ±: **10-100x daha hÄ±zlÄ±** (preprocessing sayesinde)
- Grammar bias: **100-1000x daha hÄ±zlÄ±** (vectorization sayesinde)
- GPU utilization: **Daha yÃ¼ksek** (veri beklemek yerine sÃ¼rekli hesaplama)

## ğŸ“š Dosya YapÄ±sÄ±

```
src/
â”œâ”€â”€ morpho_splitter.py      # Morfem ayrÄ±mÄ±
â”œâ”€â”€ morphopiece.py          # MorphoPiece tokenizer
â”œâ”€â”€ agglutinative_attention.py  # SOV attention (morpho_types support)
â”œâ”€â”€ grammar_engine.py       # Dilbilgisi kurallarÄ± (vectorized)
â”œâ”€â”€ tma1_model.py           # TMA-1 model
â””â”€â”€ dataset.py              # Dataset (JSONL + morpho_types support)

scripts/
â”œâ”€â”€ preprocess_for_tma1.py  # Preprocessing script (NEW)
â”œâ”€â”€ make_test_corpus.py     # Test corpus oluÅŸturma
â””â”€â”€ ...
```

---

**Sahiplik ve Lisans:** Bu proje ve modeller Tevfik Ä°ÅŸkÄ±n'a aittir. AyrÄ±ntÄ±lar iÃ§in `LICENSE.md`.

**"TÃ¼rkÃ§e'nin eklemeli yapÄ±sÄ± = Model'in DNA'sÄ±"** ğŸš€

