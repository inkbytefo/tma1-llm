// Developer: inkbytefo
// AI: Trae Coding Assistant
// Modified: 2025-11-01

# ğŸš€ TMA-1 Quick Start - MorphoPiece Tokenizer EÄŸitimi

## HÄ±zlÄ± BaÅŸlangÄ±Ã§

```bash
# KÃ¼Ã§Ã¼k veriyle hÄ±zlÄ± kurulum
python scripts/make_test_corpus.py

# AdÄ±m adÄ±m
python src/train_morphopiece.py --preprocess   --corpus-file data/test_corpus.txt --preprocessed-file data/corpus_morpho_processed.txt
python src/train_morphopiece.py --train        --preprocessed-file data/corpus_morpho_processed.txt --output tokenizer/morphopiece --vocab-size 1000
```

## DetaylÄ± KullanÄ±m

### 1. Veri Ä°ndirme (1.5 GB)

```bash
# Opsiyonel: BÃ¼yÃ¼k corpus indirme (internet ve disk gerektirir)
python src/train_morphopiece.py --download --corpus-file data/corpus_combined.txt
```

**Ã‡Ä±ktÄ±:**
- `data/mc4_turkish.txt` (0.75 GB)
- `data/wikipedia_turkish.txt` (0.75 GB)
- `data/corpus_combined.txt` (1.5 GB)

### 2. Morfem AyrÄ±mÄ± ile Ã–n Ä°ÅŸleme

```bash
# Morfem ayrÄ±mÄ± (Zemberek varsa Java ile, yoksa regex fallback)
python src/train_morphopiece.py --preprocess --corpus-file data/corpus_combined.txt --preprocessed-file data/corpus_morpho_processed.txt
```

**Ä°ÅŸlem:**
- Her kelime â†’ kÃ¶k + ekler
- Ã–rnek: "Evlerimdekiler" â†’ "ev ler im de ki ler"

**Ã‡Ä±ktÄ±:**
- `data/corpus_morpho_processed.txt` (morfem ayrÄ±mÄ± yapÄ±lmÄ±ÅŸ)

### 3. MorphoPiece Tokenizer EÄŸitimi

```bash
# SentencePiece ile tokenizer eÄŸit
python src/train_morphopiece.py --train --preprocessed-file data/corpus_morpho_processed.txt --output tokenizer/morphopiece --vocab-size 32000 --model-type unigram --character-coverage 1.0
```

**Parametreler:**
- `vocab_size=32000`: Vocabulary boyutu
- `model_type='unigram'`: Unigram modeli
- `character_coverage=1.0`: Tam karakter kapsamÄ±

**Ã‡Ä±ktÄ±:**
- `tokenizer/morphopiece.model` - SentencePiece model
- `tokenizer/morphopiece.vocab` - Vocabulary dosyasÄ±
- `tokenizer/morphopiece_vocab.json` - JSON format vocabulary

## KullanÄ±m

```python
from src.morphopiece import MorphoPiece

# Tokenizer yÃ¼kle
morphopiece = MorphoPiece("tokenizer/morphopiece.model")

# Morfem-aware encoding
tokens = morphopiece.encode(
    "DÃ¼n markete gittim",
    morpho_aware=True,
    out_type=int
)
# Output: [1234, 5678, 9012, ...]  # KÃ¶k ve ekler ayrÄ± token'lar

# Decoding
text = morphopiece.decode(tokens)
# Output: "DÃ¼n markete gittim"
```

## Ã–zellikler

âœ… **Morfem AyrÄ±mÄ±**: Zemberek ile kÃ¶k + ek ayrÄ±mÄ±  
âœ… **KÃ¶k = AyrÄ± Token**: KÃ¶kler ayrÄ± token olarak saklanÄ±r  
âœ… **Ek = AyrÄ± Token**: Ekler ayrÄ± token olarak saklanÄ±r  
âœ… **1.5 GB Corpus**: MC4 + Wikipedia TÃ¼rkÃ§e  
âœ… **32k Vocab**: 32,000 token vocabulary  
âœ… **Unigram Model**: SentencePiece unigram algoritmasÄ±  
âœ… **Character Coverage 1.0**: TÃ¼m karakterleri kapsar  

## SÃ¼re Tahmini

| AdÄ±m | SÃ¼re |
|------|------|
| Veri Ä°ndirme | 10-30 dk (internet hÄ±zÄ±na baÄŸlÄ±) |
| Morfem Ã–n Ä°ÅŸleme | 30-60 dk (corpus boyutuna baÄŸlÄ±) |
| Tokenizer EÄŸitimi | 10-30 dk (CPU'ya baÄŸlÄ±) |
| **TMA-1 Preprocessing** (YENÄ°) | 20-40 dk (corpus boyutuna baÄŸlÄ±) |
| **Toplam (Tokenizer)** | **50-120 dk** |
| **TMA-1 Model EÄŸitimi** | DeÄŸiÅŸken (epoch sayÄ±sÄ±, corpus boyutu, GPU) |

**Not**: TMA-1 preprocessing yapÄ±ldÄ±ÄŸÄ±nda eÄŸitim sÃ¼resi **10-100x azalÄ±r** (runtime morfolojik analiz yok).

## Notlar

- Ä°lk Ã§alÄ±ÅŸtÄ±rmada internet baÄŸlantÄ±sÄ± gerekli (veri indirme)
- Morfem analizi zaman alabilir (1.5 GB corpus)
- Tokenizer eÄŸitimi CPU-intensive (multi-threading kullanÄ±r)
- Ã‡Ä±ktÄ± dosyalarÄ± ~100-200 MB olabilir

## Sorun Giderme

### "datasets not found"
```bash
pip install datasets
```

### "sentencepiece not found"
```bash
pip install sentencepiece
```

### "Memory error"
- `--max-lines` parametresiyle satÄ±r sayÄ±sÄ±nÄ± sÄ±nÄ±rlayÄ±n
- Ã–rnek: `--max-lines 1000000`

### "Download timeout"
- Daha kÃ¼Ã§Ã¼k corpus boyutu deneyin
- Ã–rnek: `--mc4-size 0.5 --wikipedia-size 0.5`

## TMA-1 Model EÄŸitimi

### HÄ±zlÄ± EÄŸitim (Ã–nerilen - Optimize EdilmiÅŸ)

```bash
# 1. Corpus Ã¶n iÅŸleme (morfolojik analiz - BÄ°R KEZ)
python scripts/preprocess_for_tma1.py \
    --input data/corpus_morpho_processed.txt \
    --output data/train_data.jsonl \
    --tokenizer tokenizer/morphopiece.model

# 2. TMA-1 eÄŸitimi (Ã¶n iÅŸlenmiÅŸ JSONL ile - HIZLI)
python train_tma1.py \
    --corpus data/train_data.jsonl \
    --tokenizer tokenizer/morphopiece.model \
    --output-dir models/tma1 \
    --batch-size 8 \
    --learning-rate 3e-4
```

**Ã–nemli**: JSONL formatÄ± kullanÄ±ldÄ±ÄŸÄ±nda morfolojik analiz eÄŸitim sÄ±rasÄ±nda yapÄ±lmaz, bu da **10-100x hÄ±zlanma** saÄŸlar.

### Eski YÃ¶ntem (YavaÅŸ - Sadece Test Ä°Ã§in)

```bash
# Text formatÄ± kullanÄ±lÄ±rsa runtime morfolojik analiz yapÄ±lÄ±r (YAVAÅ)
python train_tma1.py \
    --corpus data/corpus_morpho_processed.txt \
    --tokenizer tokenizer/morphopiece.model \
    --output-dir models/tma1
```

## Sonraki AdÄ±mlar

1. âœ… MorphoPiece tokenizer hazÄ±r
2. âœ… **Corpus preprocessing** (`preprocess_for_tma1.py`)
3. âœ… Testler (`pytest -q`)
4. ğŸ”„ TMA-1 model eÄŸitimi (`train_tma1.py` ile - **JSONL format kullanÄ±n**)
5. ğŸ”„ Inference testi (`llm_engine.py` ile)

---

**"Morfem farkÄ±ndalÄ±ÄŸÄ± = TÃ¼rkÃ§e'nin DNA'sÄ±"** ğŸ§¬

